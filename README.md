<<<<<<< Updated upstream
# hSNN â€” Spiking Hypergraph Neural Network
## Current State
 present state of the  workspace, with emphasis on the CLIâ€‘first workflow, Neuromorphic IR compatibility (NIR), verification and lowering via the compiler, and minimal visualization.

General/Paradigm: CLIâ€‘first neuromorphic research substrate that treats the command line as the primary UX. Everything else (IR, runtime, storage, visualization) supports that workflow.
Thinâ€‘waist architecture: A stable, versioned, unitâ€‘aware IR and binary storage schemas form the longâ€‘lived â€œwaistâ€ between highâ€‘level tools and lowâ€‘level engines. Explicit trait boundaries and narrow, composable contracts.
Endâ€‘toâ€‘end NIR path: Textual NIR â†’ parse â†’ verify â†’ (passes) â†’ lower â†’ run â†’ export â†’ visualize.
Core components

shnn-ir (Neuromorphic IR)
Purpose: Define the textual IR data model and tooling.
Implemented:
Module/Operation model with dialects (neuron, plasticity, connectivity, stimulus, runtime).
Versioned (name@vN), typed, unitâ€‘aware attributes:
TimeNs, DurationNs, VoltageMv, ResistanceMohm, CapacitanceNf, CurrentNa, RateHz, Weight, RangeU32, NeuronRef, plus scalars.
Textual printer (to_text) and minimal parser (parse_text) with roundâ€‘trip parity (tests included).
Convenience constructors (sugar) for common ops:
neuron.lif@v1, plasticity.stdp@v1, connectivity.layer_fully_connected@v1, connectivity.synapse_connect@v1, stimulus.poisson@v1, runtime.simulate.run@v1.
Tests:
Singleâ€‘op and mixed module roundâ€‘trip tests (including synapse_connect).
Conformance parity tests used downstream by the compiler.
shnn-compiler (NIR compiler)
Purpose: Verify NIR, optionally transform via passes, and lower to the runtime engine.
Public API:
verify_module(&Module) â†’ Result<()>: Ensures attribute presence, types/units, and semantic bounds (e.g., tau_m > 0, r_m > 0, c_m > 0; stdp tau > 0 and w_min â‰¤ w_max; range validity; dt/duration > 0; etc.).
list_ops() â†’ &'static [OpSpec]: Access the static op registry (dialect, name, version, attributes) for dynamic introspection.
compile_with_passes(&Module) â†’ Result:
Pipeline: verify â†’ run passes â†’ lower â†’ runnable SimulationEngine.
Pass framework (crates/shnn-compiler/src/passes.rs):
CanonicalizePass: expands connectivity.layer_fully_connected into explicit connectivity.synapse_connect ops; normalizes attributes where appropriate.
UpgradeVersionsPass: scaffolding for automatic inâ€‘place upgrades (e.g., lif@v0 â†’ lif@v1) with defaulted attributes.
Lowering:
Sets LIF/STDP defaults (NetworkConfig), builds neurons/synapses via NetworkBuilder, collects StimulusPattern values, and configures SimulationParams for the runtime engine.
Tests:
verify_bounds.rs: negative/positive coverage for semantics and unit checks.
compile_with_passes_ok.rs: endâ€‘toâ€‘end compile (with passes) and run.
nir_parity_roundtrip.rs: compileâ†’run parity comparing direct Module vs parse(text(Module)).
Op Registry (public â€œOpRegistryâ€)

Backed by static OpSpec/AttributeSpec entries with AttrKind enumeration (e.g., DurationNs, VoltageMv, etc.).
Drives:
CLI dynamic op listing (printing attribute names, kinds, required/optional, and docs).
Verification (type checks and presence).
Documentation and future JSON serialization of op schemas.
shnn-runtime (Engine)
Purpose: Execute the lowered program.
Current:
NetworkBuilder and SNN network construction with LIF neurons.
Optional STDP plasticity configuration.
SimulationEngine capable of running a program and exporting spikes.
Integration: The compiler lowers IR ops into runtime constructs (network configuration, synapses, stimuli, simulation parameters).
shnn-cli (Command Line Interface)
Purpose: The primary user entrypoint.
NIR commands:
snn nir compile â€” Construct textual NIR from CLI settings (LIF defaults, optional STDP, fullyâ€‘connected layers, Poisson stimuli, simulate.run).
snn nir verify â€” Parse and verify a textual NIR file.
snn nir run â€” Parse â†’ verify â†’ compile_with_passes â†’ run. Optionally export spike JSON (time_ns/time_ms + neuron_id).
snn nir op-list [--detailed] â€” Dynamically print supported dialects/ops/versions and attribute schemas from shnn-compiler::list_ops(), using AttrKind::name() for type names.
Visualization:
snn viz serve â€” Starts a small static file server (no extra deps) hosting an SPA with a canvas spike raster.
Endpoints: / (SPA), /api/health, /api/list (list JSON files in --results-dir), /api/spikes?file=...
SPA reads exported spike JSON and renders a raster (Canvas 2D), structured for future WebGL2.
Additional scaffolds:
TTR command: parses a TOML â€œprogramâ€ and outputs a JSON mask (placeholder, to migrate to storage VMSK).
Study runner: parses a TOML study config and orchestrates sequential runs (exports perâ€‘run summaries and summary.json).
Storage layer (design complete; staged implementation)
Binary schemas (documented): VCSR, VEVT, VMSK, VMORF, VGRF, VRAS; targeted at zeroâ€‘copy or minimalâ€‘copy IO and typed headers.
Traits (â€œinterfacesâ€): Stable contracts for reading/writing and interop with the runtime and IR layers.
Current code includes schema utilities and partial implementations; JSON is used for early viz exports.
Thinâ€‘waist traits (â€œinterfacesâ€)
Stable boundaries define contracts across:
IR (ops/types/units and registry)
Compiler (verify, passes, lowering)
Runtime (network construction, stimuli, simulation)
Storage (binary schemas)
Ensures that tools can evolve independently while preserving reproducibility and determinism.
Current user flows (endâ€‘toâ€‘end)

Introspect ops:
cargo run -p shnn-cli -- nir op-list --detailed
Compile a textual NIR:
cargo run -p shnn-cli -- nir compile --output /tmp/demo.nirt
Run a textual NIR and export spikes:
cargo run -p shnn-cli -- nir run /tmp/demo.nirt --output crates/shnn-cli/test_workspace/results/run1.json
Visualize spikes:
cargo run -p shnn-cli -- viz serve --results-dir crates/shnn-cli/test_workspace/results
Open the printed http://127.0.0.1:7878
Quality and testing

IR roundâ€‘trip tests (printâ†’parseâ†’print).
Compiler bounds/semantic verifier tests (negative/positive).
Compileâ†’run parity tests (direct Module vs parse(text(Module))).
CLI remains minimalâ€‘dep and runs on stable toolchains.
Whatâ€™s not done yet (high level)

Rich canonicalization and version upgrades beyond v1 examples.
Topology/measurement/analytics endpoints and visualization in the SPA (WebGL2 path).
Binary mask writing (VMSK) and workloadâ€‘aware transforms (TTR engine).
Parallelized, resumable study orchestration and robust determinism harness.
References

NIR dialects and versioning: docs/architecture/NIR_DIALECTS_AND_VERSIONING.md
CLI commands: crates/shnn-cli/src/commands
Compiler public API: crates/shnn-compiler/src/lib.rs and src/passes.rs
CLIâ€‘first neuromorphic research "substrate" for building, verifying, compiling, running, and visualizing spiking neural networks (SNNs), emphasizing ease, reproducibility, and flexibility. 

Status: Core foundations are mostly implemented and tested. Working on figuring out the (NIR) path most erthing else is live through the compiler and CLI, Gotta modularize the MLIR shit..... dynamic op introspection, verification, lowering to runtime, simulation, JSON exports, adding a minimal visualization server with spike raster rendering. Several advanced features are scaffolded for incremental, compilationâ€‘safe expansion.

Next up"
- Textual NIR with versioned, typed, unitâ€‘aware ops, MLIRâ€‘like syntax and roundâ€‘trip printing/parsing
- Static, introspectable Op Registry with attribute kinds and docs
- Compiler with verification, lowering, and a pass framework (canonicalization and version upgrader)
- CLI add NIR commands (compile, run, verify, opâ€‘list), and a minimal viz server (serve + JSON endpoints)
- Storage and runtime foundations aligned to thinâ€‘waist traits and zeroâ€‘copy binary schemas (design complete; select pieces implemented)
=======
# Welcome to the Future of Computing ğŸ§ âš¡

## What if your computer could think like a brain?

Imagine a computer that doesn't crunch numbers in rigid steps, but instead sends tiny electrical spikes that carry informationâ€”just like neurons in your brain. This isn't science fiction. This is **neuromorphic computing**, and you're about to discover why it's going to change everything.

## Your Brain vs. Your Laptop: A Tale of Two Processors
>>>>>>> Stashed changes

Right now, your laptop burns through 65 watts to run a video call. Your brain? It does everythingâ€”seeing, thinking, remembering, dreamingâ€”on just 20 watts. That's less power than a light bulb.

The secret? Your brain doesn't process information like a traditional computer. Instead of moving data back and forth between memory and processor (the infamous "von Neumann bottleneck"), your brain's 86 billion neurons communicate directly through **spikes**â€”brief electrical pulses that carry both data and computation together.

## Traditional AI vs. Neuromorphic: The Ultimate Showdown

**Traditional AI:**
- ğŸ”¥ Burns massive amounts of energy (ChatGPT uses as much power as a small city)
- ğŸŒ Processes everything in batches, even for real-time tasks
- ğŸ§± Rigid architectures that require complete retraining for new tasks
- ğŸ’¸ Needs expensive GPUs and cloud infrastructure

**Neuromorphic Computing:**
- âš¡ Ultra-low power (1000x more efficient for many tasks)
- ğŸƒâ€â™‚ï¸ Processes information as it arrives, in real-time
- ğŸ§  Learns continuously, adapting to new patterns on the fly
- ğŸ’¡ Runs on tiny chips, even in your smartphone

## Why Everyone Else Makes This Hard (And We Don't)

Most neuromorphic platforms are academic nightmares:
- ğŸ“š Require PhD-level neuroscience knowledge just to get started
- ğŸ”§ Need custom hardware and proprietary tools
- ğŸ•¸ï¸ Tangled codebases with zero documentation
- ğŸ¯ Focus on recreating biological accuracy instead of solving real problems

**We took a different approach.**

## Introducing hSNN: Neuromorphic Computing Made Human

hSNN is the first neuromorphic platform designed for **developers, not just neuroscientists**. Here's what makes us different:

### ğŸ¯ CLI-First Philosophy
```bash
# Want to build a neuromorphic network? Three commands:
snn nir compile --output my-network.nirt
snn nir run my-network.nirt --output results.json
snn viz serve --results-dir ./results
```

No IDEs to learn. No proprietary tools. Just simple commands that do exactly what they say.

### ğŸ—ï¸ "Thin Waist" Architecture
We built the stable foundation everyone else forgot:
- **Neuromorphic IR (NIR)**: A universal language for describing spiking networks
- **Binary schemas**: Lightning-fast data formats that work everywhere
- **Trait-based interfaces**: Swap components without breaking anything

Think of it as the "HTML of neuromorphic computing"â€”a common standard that just works.

### ğŸ”„ Round-Trip Everything
```nir
# Write your network in human-readable NIR:
%lif1 = neuron.lif<v_th=1.0, v_reset=0.0>() -> (3,)
%syn1 = connectivity.fully_connected<weight=0.5>(%lif1) -> (3,)
```

Our system can convert this to binary, simulate it, visualize it, and convert it back to text **without losing any information**. Try that with TensorFlow.

### ğŸ® Real-Time Visualization
See your spiking networks come alive with built-in visualization:
- ğŸ“Š Spike raster plots showing neuron activity over time
- ğŸ•¸ï¸ Network topology viewers
- ğŸ“ˆ Real-time performance metrics

### ğŸ”§ Extensible by Design
- **Static introspection**: Query what operations are available at compile time
- **Pass framework**: Transform and optimize networks with composable passes
- **Multiple backends**: CPU, embedded, WASM, Pythonâ€”one codebase, many targets

## Real-World Applications You Can Build Today

- ğŸ¤– **Autonomous drones** that navigate using 100x less power
- ğŸ‘ï¸ **Computer vision** systems that work in real-time on mobile devices
- ğŸ§ **Audio processing** that adapts to your environment
- ğŸ•¹ï¸ **Game AI** that learns your playing style
- ğŸ  **Smart sensors** that run for years on a single battery

## The Science Made Simple

### Spikes: Nature's Information Packets
Traditional computers represent information as continuous numbers (0.7, 3.14, etc.). Brains use **spikes**â€”binary events that happen at specific times. It's like the difference between a dimmer switch and a telegraph key.

<<<<<<< Updated upstream
Citation
If you use hSNN in your research, please cite:
```bibtex
@software{hsnn2025,
  title   = {hSNN: Spiking Hypergraph Neural Network},
  author  = {hSNN Development Team},
  year    = {2025},
  url     = {https://github.com/hsnn-project/hsnn},
  version = {0.1.0}
}
=======
### Time is Everything
In traditional AI, time is just another dimension. In neuromorphic computing, **time IS the computation**. When spikes arrive, what they do, and how neurons respondâ€”that's where the magic happens.

### Learning Without Forgetting
Your brain doesn't retrain from scratch every time you learn something new. Neuromorphic systems use **spike-timing dependent plasticity (STDP)**â€”connections get stronger or weaker based on the precise timing of spikes. It's learning that happens naturally, without backpropagation.

## Why This Matters Now

We're hitting the limits of traditional computing:
- **Moore's Law is dead**: Transistors can't get much smaller
- **Power walls**: Data centers consume 4% of global electricity
- **The AI bubble**: Current approaches don't scale to real-world deployment

Neuromorphic computing isn't just the next stepâ€”it's the only step that makes sense.

## Your Journey Starts Here

This isn't just documentation. This is your guide from zero to neuromorphic hero. We'll take you from "What's a spike?" to building and deploying production neuromorphic systems.

Whether you're:
- ğŸ“ A student curious about brain-inspired computing
- ğŸ‘©â€ğŸ’» A developer wanting to build ultra-efficient AI
- ğŸ”¬ A researcher exploring new computational paradigms
- ğŸ¢ An engineer solving real-world power and latency problems

...you're in the right place.

---

## The Challenge ğŸ¯

**We dare you to build a neuromorphic system in the next hour.**

No PhD required. No expensive hardware. No months of setup.

Just you, this documentation, and the future of computing.

**[Accept the Challenge â†’ Start with Quick Start](Morphics/quick-start.md)**

*Or if you want to understand the "why" before the "how":*
**[Understand the Science â†’ Introduction to Neuromorphic Computing](Morphics/introduction/)**

---

## Quick Navigation

- ğŸš€ **[Quick Start](Morphics/quick-start.md)** - Build your first neuromorphic network in minutes
- ğŸ§  **[Introduction](Morphics/introduction/)** - Neuromorphic computing explained for humans
- ğŸ› ï¸ **[CLI Workflows](Morphics/cli-workflows/)** - Master the command-line tools
- ğŸ—ï¸ **[Architecture](Morphics/architecture/)** - The "thin waist" that makes it all work
- âš™ï¸ **[Engine](Morphics/engine/)** - Under the hood of the simulation engine
- ğŸ”Œ **[Interop](Morphics/interop/)** - Python, WASM, embedded, and more
- ğŸ‘¥ **[Contributing](Morphics/contributing/)** - Join the neuromorphic revolution

---

*"The best way to predict the future is to implement it."* - hSNN Development Team
>>>>>>> Stashed changes
